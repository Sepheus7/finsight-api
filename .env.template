# FinSight Configuration Template
# Copy this file to .env and fill in your values

# LLM Provider Configuration
FINSIGHT_LLM_PROVIDER=ollama                # Options: auto, ollama, openai, anthropic, bedrock, regex
OLLAMA_BASE_URL=http://localhost:11434      # Ollama server URL
OLLAMA_MODEL=llama3.1:8b                    # Available models: llama3.1:8b, llama3.2:3b, codellama, etc.
OPENAI_API_KEY=your_openai_api_key_here     # Get from https://platform.openai.com/ (optional)
ANTHROPIC_API_KEY=your_anthropic_key_here   # Get from https://console.anthropic.com/ (optional)

# Model Configuration
FINSIGHT_OLLAMA_MODEL=llama3.1:8b           # Recommended for local inference
FINSIGHT_OPENAI_MODEL=gpt-4o-mini           # Fallback option
FINSIGHT_ANTHROPIC_MODEL=claude-3-haiku-20240307
FINSIGHT_BEDROCK_MODEL=anthropic.claude-3-haiku-20240307-v1:0    # AWS Bedrock primary model
FINSIGHT_BEDROCK_FALLBACK_MODEL=amazon.titan-text-express-v1      # Cost-optimized fallback (67% cheaper)
FINSIGHT_BEDROCK_REGION=us-east-1           # AWS region for Bedrock
FINSIGHT_TEMPERATURE=0.1                    # Lower = more deterministic
FINSIGHT_MAX_TOKENS=1000

# System Configuration
FINSIGHT_DEBUG=false                        # Set to true for detailed logging
FINSIGHT_CACHE_ENABLED=true                 # Enable caching for better performance
FINSIGHT_CACHE_HOURS=24                     # Cache duration in hours
FINSIGHT_MAX_RETRIES=3                      # Max retries for API calls
FINSIGHT_REQUEST_TIMEOUT=30                 # Request timeout in seconds

# AWS Configuration (for Lambda deployment)
AWS_REGION=us-east-1
S3_BUCKET=your-s3-bucket-name
LAMBDA_TIMEOUT=300
LAMBDA_MEMORY=1024

# Data Sources
YAHOO_FINANCE_ENABLED=true
SEC_EDGAR_ENABLED=false                     # Future enhancement
